import tensorflow as tf
import numpy as np
import gym


class A2C:
    def __init__(self):
        self.game = gym.make('CartPole-v1')
        self.num_actions = self.game.action_space.n
        self.state_size = self.game.observation_space.shape[0]

        self.state_input = tf.placeholder(tf.float32, [None, self.state_size])

        # Define any additional placeholders needed for training your agent here:
        self.rwd = tf.placeholder(shape=[None], dtype=tf.float32)
        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)
        self.state_value = self.critic()
        self.actor_probs = self.actor()
        self.loss_val = self.loss()
        self.train_op = self.optimizer()
        self.session = tf.Session()
        self.session.run(tf.global_variables_initializer())

    def optimizer(self):
        """
        :return: Optimizer for your loss function
        """
        return tf.train.AdamOptimizer(1e-3).minimize(self.loss_val)

    def critic(self):
        """
        Calculates the estimated value for every state in self.state_input. The critic should not depend on
        any other tensors besides self.state_input.
        :return: A tensor of shape [num_states] representing the estimated value of each state in the trajectory.
        """
        state = self.state_input
        hiddensz = 40
        W = tf.Variable(tf.random_uniform([self.state_size, hiddensz], dtype=tf.float32))
        hidden = tf.nn.relu(tf.matmul(state, W))
        O = tf.Variable(tf.random_uniform([hiddensz, 1], dtype=tf.float32))
        output = tf.matmul(hidden, O)

        return output

    def actor(self):
        """
        Calculates the action probabilities for every state in self.state_input. The actor should not depend on
        any other tensors besides self.state_input.
        :return: A tensor of shape [num_states, num_actions] representing the probability distribution
            over actions that is generated by your actor.
        """
        state = self.state_input
        hiddensz = 40
        W = tf.Variable(tf.random_uniform([self.state_size, hiddensz], dtype=tf.float32))
        hidden = tf.nn.relu(tf.matmul(state, W))
        O = tf.Variable(tf.random_uniform([hiddensz, self.num_actions], dtype=tf.float32))
        output = tf.nn.softmax(tf.matmul(hidden, O))

        return output

    def loss(self):
        """
        :return: A scalar tensor representing the combined actor and critic loss.
        """
        indicies = tf.range(0, tf.shape(self.actor_probs)[0]) * 2 + self.actions
        actProbs = tf.gather(tf.reshape(self.actor_probs, [-1]), indicies)
        lossa = -tf.reduce_mean(tf.log(actProbs)*(self.rwd-self.state_value))
        lossc = tf.reduce_mean(tf.square(self.rwd - self.state_value))
        return lossa + lossc

    def train_episode(self):
        """
        train_episode will be called 1000 times by the autograder to train your agent. In this method,
        run your agent for a single episode, then use that data to train your agent. Feel free to
        add any return values to this method.
        """
        state = self.game.reset()
        actprob = self.actor_probs
        critic = self.state_value
        sess = self.session
        game = self.game
        train_op = self.train_op
        rTot = []
        gamma = 0.99
        sts = []
        ans = []
        rs = []
        for j in range(999):
            nActs, crival = sess.run([actprob, critic], feed_dict={self.state_input: [state]})
            nAct = np.random.choice(self.num_actions, p=nActs[0])
            a = np.array(self.num_actions)
            s1, rwd, dn, _ = game.step(nAct)
            rs.append(rwd)
            sts.append(state)
            ans.append(nAct)
            state = s1
            if dn:
                disRs = np.zeros_like(rs)
                add = 0
                for t in reversed(range(0, j + 1)):
                    add = add * gamma + rs[t]
                    disRs[t] = add
                sess.run(train_op, feed_dict={self.state_input: sts, self.actions: ans, self.rwd: disRs})
                rTot.append(j)
                break
        return rTot


def check_actor(model):
    """
    The autograder will use your actor() function to test your agent. This function
    checks that your actor returns a tensor of the right shape for the autograder.
    :return: True if the model's actor returns a tensor of the correct shape.
    """
    dummy_state = np.ones((10, 4))
    actor_probs = model.session.run(model.actor_probs, feed_dict={
        model.state_input: dummy_state
    })
    return actor_probs.shape == (10, 2)


if __name__ == '__main__':
    # Change __main__ to train your agent for 1000 episodes and print the average reward over the last 100 episodes.
    # The code below is similar to what our autograder will be running.

    learner = A2C()
    r = []
    for i in range(1000):
        rTot = learner.train_episode()
        r.append(rTot)
    assert(check_actor(learner))
    print("Average reward : ", np.average(r[:]))

